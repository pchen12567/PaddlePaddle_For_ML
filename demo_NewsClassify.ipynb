{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: 导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "import shutil\n",
    "import paddle\n",
    "import paddle.fluid as fluid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- paddle.fluid 飞桨核心框架\n",
    "- multiprocessing python中多进程管理包\n",
    "- numpy 科学计算模块\n",
    "- shutil 文本操作模块\n",
    "- os 操作系统模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据集**：\n",
    "- 爬虫爬取中文文本数据\n",
    "- 爬取56821条数据中文新闻标题\n",
    "- 10个类别：国际、文化、娱乐、体育、财经、汽车、教育、科技、房产、证券"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置各类数据文件目录\n",
    "data_root_path = './text_data/'\n",
    "dict_path = os.path.join(data_root_path, 'dict_txt.txt')\n",
    "data_path = os.path.join(data_root_path, 'news_classify_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 创建数据字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(data_path, dict_path):\n",
    "    # 初始化容器集合\n",
    "    dict_set = set()\n",
    "    \n",
    "    # 按行读取原始数据\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        # 获取当前新闻的标题\n",
    "        title = line.split('_!_')[-1].replace('\\n', \"\")\n",
    "        \n",
    "        # 将当前新闻标题中的每个字依次存放到集合容器中\n",
    "        for s in title:\n",
    "            dict_set.add(s)\n",
    "    \n",
    "    # 初始化字典列表\n",
    "    dict_list = []\n",
    "    \n",
    "    # 初始化序号\n",
    "    i = 0\n",
    "    \n",
    "    # 给集合容器中的每个字分配序号依次存入到字典列表中\n",
    "    for s in dict_set:\n",
    "        dict_list.append([s, i])\n",
    "        i += 1\n",
    "        \n",
    "    # 根据字典列表生成字典    \n",
    "    dict_txt = dict(dict_list)\n",
    "    \n",
    "    # 最后手动加入字典 {<unk> : end_id}\n",
    "    end_dict = {\"<unk>\": i}\n",
    "    dict_txt.update(end_dict)\n",
    "    \n",
    "    # 将生成好的数据字典写入文件保存\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(dict_txt))\n",
    "    \n",
    "    print(\"数据字典生成完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据字典生成完成！\n"
     ]
    }
   ],
   "source": [
    "create_dict(data_path, dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据原始数据集创建序列化表示的数据\n",
    "def create_data_list(data_root_path):\n",
    "    # 新建测试文件数据集\n",
    "    with open(data_root_path + 'test_list.txt', 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    # 新建训练文件数据集\n",
    "    with open(data_root_path + 'train_list.txt', 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    # 按行读取数据字典文件并转换为字典类型\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f_data:\n",
    "        dict_txt = eval(f_data.readlines()[0])\n",
    "    \n",
    "    # 按行读取原始数据\n",
    "    with open(data_path, 'r', encoding = 'utf-8') as f_data:\n",
    "        lines = f_data.readlines()\n",
    "    \n",
    "    # 初始化序号\n",
    "    i = 0\n",
    "    \n",
    "    # 按行读取原始数据\n",
    "    for line in lines:\n",
    "        # 获取当前数据新闻标题\n",
    "        title = line.split('_!_')[-1].replace('\\n', '')\n",
    "        \n",
    "        # 获取当前数据新闻分类标签\n",
    "        lab = line.split('_!_')[1]\n",
    "        \n",
    "        # 初始化当前数据新闻标题数字ids\n",
    "        title_ids = \"\"\n",
    "        \n",
    "        # 每10条数据选取一条作为测试用数据\n",
    "        if i % 10 == 0:\n",
    "            with open(os.path.join(data_root_path, 'test_list.txt'), 'a', encoding='utf-8') as f_test:\n",
    "                for word in title:\n",
    "                    # 获取当前标题中每个字在数据字典中对应的id\n",
    "                    word_id = str(dict_txt[word])\n",
    "                    \n",
    "                    # 拼接每一个字对应的id形成标题的完整ids，中间用逗号隔开\n",
    "                    title_ids = title_ids + word_id + ','\n",
    "                \n",
    "                # 拼接完成当前新闻标题的数字ids后，去掉末尾的逗号\n",
    "                title_ids = title_ids[:-1]\n",
    "                \n",
    "                # 将当前新闻标题的数字ids 和 新闻分类标签拼接起来，中间用 table 隔开\n",
    "                title_ids = title_ids + '\\t' + lab + '\\n'\n",
    "                \n",
    "                # 写入测试文件列表\n",
    "                f_test.write(title_ids)\n",
    "                \n",
    "        else:\n",
    "            # 其他数据作为训练数据\n",
    "            with open(os.path.join(data_root_path, 'train_list.txt'), 'a', encoding='utf-8') as f_train:\n",
    "                for word in title:\n",
    "                    # 获取当前标题中每个字在数据字典中对应的id\n",
    "                    word_id = str(dict_txt[word])\n",
    "                    \n",
    "                    # 拼接每一个字对应的id形成标题的完整ids，中间用逗号隔开\n",
    "                    title_ids = title_ids + word_id + ','\n",
    "                \n",
    "                # 拼接完成当前新闻标题的数字ids后，去掉末尾的逗号\n",
    "                title_ids = title_ids[:-1]\n",
    "                \n",
    "                # 将当前新闻标题的数字ids 和 新闻分类标签拼接起来，中间用 table 隔开\n",
    "                title_ids = title_ids + '\\t' + lab + '\\n'\n",
    "                \n",
    "                # 写入训练文件列表\n",
    "                f_train.write(title_ids)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    print('数据列表生成完成！')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据列表生成完成！\n"
     ]
    }
   ],
   "source": [
    "create_data_list(data_root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 训练/测试数据的预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建data_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mapper(sample):\n",
    "    data, label = sample\n",
    "    data = [int(d) for d in data.split(',')]\n",
    "    \n",
    "    return data, int(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建训练数据提取器 train_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reader(train_list_path):\n",
    "    \n",
    "    def reader():\n",
    "        # 按行从训练数据列表中读取数据\n",
    "        with open(train_list_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            # 打乱数据\n",
    "            np.random.shuffle(lines)\n",
    "            \n",
    "            # 获取每条新闻标题的数字ids和分类标签\n",
    "            for line in lines:\n",
    "                data, label = line.split('\\t')\n",
    "                yield data, label\n",
    "                \n",
    "    return paddle.reader.xmap_readers(data_mapper, reader, cpu_count(), 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建测试数据提取器 test_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reader(test_list_path):\n",
    "    \n",
    "    def reader():\n",
    "        with open(test_list_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                data, label = line.split('\\t')\n",
    "                yield data, label\n",
    "                \n",
    "    return paddle.reader.xmap_readers(data_mapper, reader, cpu_count(), 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paddle.reader.xmap_readers():通过多线程方式，通过用户自定义的映射器mapper来映射reader返回的样本（到输出队列)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: 配置神经网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积神经网络（Convolutional Neural Networks, CNN）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入词向量序列，产生一个特征图（feature map），对特征图采用时间维度上的最大池化（max pooling over time）操作得到此卷积核对应的整句话的特征，最后，将所有卷积核得到的特征拼接起来即为文本的定长向量表示，对于文本分类问题，将其连接至softmax即构建出完整的模型。\n",
    "\n",
    "在实际应用中，使用多个卷积核来处理句子，窗口大小相同的卷积核堆叠起来形成一个矩阵，这样可以更高效的完成运算。\n",
    "\n",
    "另外，也可使用窗口大小不同的卷积核来处理句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/pchen12567/picture_store/blob/master/PaddlePaddle/text_classify_001.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 创建CNN网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义的网络结构是：输入层-->卷积与池化层-->输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_net(data, dict_dim, class_dim=10, emb_dim=128, hid_dim=128, hid_dim2=98):\n",
    "    emb = fluid.layers.embedding(input=data, size= [dict_dim, emb_dim])\n",
    "    \n",
    "    conv_3 = fluid.nets.sequence_conv_pool(\n",
    "                                            input=emb,\n",
    "                                            num_filters=hid_dim,\n",
    "                                            filter_size=3,\n",
    "                                            act='tanh',\n",
    "                                            pool_type='sqrt')\n",
    "    \n",
    "    \n",
    "    conv_4 = fluid.nets.sequence_conv_pool(\n",
    "                                            input=emb,\n",
    "                                            num_filters=hid_dim2,\n",
    "                                            filter_size=4,\n",
    "                                            act='tanh',\n",
    "                                            pool_type='sqrt')\n",
    "    \n",
    "    output = fluid.layers.fc(\n",
    "                            input=[conv_3, conv_4],\n",
    "                            size=class_dim,\n",
    "                            act='softmax')\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 定义输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 张量words: 数据类型为int64, lod_level不为0则输入数据为序列数据\n",
    "- 张量label: 代表文本分类后的类别，形状为[1]，数据类型为int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = fluid.layers.data(name='words', shape=[1], dtype='int64', lod_level=1)\n",
    "label = fluid.layers.data(name='label', shape=[1], dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 获取分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取字典的长度\n",
    "def get_dict_len(dict_path):\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "        # eval()函数将str转换为dict\n",
    "        line = eval(f.readlines()[0])\n",
    "    \n",
    "    return len(line.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dim = get_dict_len(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取分类器\n",
    "model = CNN_net(words, dict_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉熵损失函数在分类任务上比较常用\n",
    "- 定义了一个损失函数之后，还要求平均值，因为定义的是一个batch的损失值\n",
    "- 同时定义一个准确率函数，可以在训练的试函输出分类的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取损失函数和准确率函数\n",
    "cost = fluid.layers.cross_entropy(input=model, label=label)\n",
    "avg_cost = fluid.layers.mean(cost)\n",
    "acc = fluid.layers.accuracy(input=model, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 定义优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adagrad优化器**\n",
    "- 能够在训练中自动的对学习率进行调整，对于出现频率较低参数采用较大的学习了更新；\n",
    "- 相反，对于出现频率较高的参数采用较小的学习了更新，适合出来稀疏数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = fluid.optimizer.AdagradOptimizer(learning_rate=0.002)\n",
    "opt = optimizer.minimize(avg_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述模型配置完毕后，得到两个fluid.program\n",
    "\n",
    "- fluid.default_startup_program():\n",
    "    - 参数初始化操作会被写入到 fluid.default_startup_program() 中\n",
    "- fluid.default_main_program():\n",
    "    - 用于获取默认或全局 main program\n",
    "    - 该主程序用于训练和测试模型，fluid.layers 中的所有layer函数可以向 default_main_program 中添加算子和变量。\n",
    "    - 是 fluid 许多编程接口的缺省值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 获取预测程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_program = fluid.default_main_program().clone(for_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 创建训练用执行器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 指定程序运行的设备，fluid.CPUPlace()和fluid.CUDAPlace()分别表示为CPU和GPU\n",
    "- 创建一个Executor实例\n",
    "- Executor 接收传入的 program，并通过run()方法运行program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个执行器，CPU训练速度比较慢\n",
    "place = fluid.CPUPlace()  # CPU\n",
    "# place = fluid.CUDAPlace(0)  # GPU\n",
    "exe = fluid.Executor(place)\n",
    "\n",
    "# 进行参数初始化\n",
    "exe.run(fluid.default_startup_program())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 定义数据映射器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeder = fluid.DataFeeder(place=place, feed_list=[words, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 获取训练和测试数据读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_path = './text_data/train_list.txt'\n",
    "test_list_path = './text_data/test_list.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reader = paddle.batch(reader=train_reader(train_list_path), batch_size=128)\n",
    "test_reader = paddle.batch(reader=test_reader(test_list_path), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 开始训练并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练轮数\n",
    "EPOCH_NUM = 10\n",
    "\n",
    "# 设置模型保存路径\n",
    "model_save_dir = './clssify_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Cost: 2.2994, Acc: 0.1094\n",
      "Epoch: 0, Batch: 50, Cost: 1.3757, Acc: 0.6328\n",
      "Epoch: 0, Batch: 100, Cost: 1.0778, Acc: 0.6953\n",
      "Epoch: 0, Batch: 150, Cost: 0.9665, Acc: 0.7188\n",
      "Epoch: 0, Batch: 200, Cost: 0.9725, Acc: 0.7109\n",
      "Epoch: 0, Batch: 250, Cost: 0.7764, Acc: 0.7734\n",
      "Epoch: 0, Batch: 300, Cost: 0.9397, Acc: 0.6875\n",
      "Epoch: 0, Batch: 350, Cost: 0.7377, Acc: 0.7188\n",
      "Test: 0, Cost: 0.8139, Acc: 0.7391\n",
      "*************************************\n",
      "Epoch: 1, Batch: 0, Cost: 0.7662, Acc: 0.7344\n",
      "Epoch: 1, Batch: 50, Cost: 0.7176, Acc: 0.7656\n",
      "Epoch: 1, Batch: 100, Cost: 0.7560, Acc: 0.7344\n",
      "Epoch: 1, Batch: 150, Cost: 0.7311, Acc: 0.7266\n",
      "Epoch: 1, Batch: 200, Cost: 0.6404, Acc: 0.7891\n",
      "Epoch: 1, Batch: 250, Cost: 0.6908, Acc: 0.7656\n",
      "Epoch: 1, Batch: 300, Cost: 0.9653, Acc: 0.7109\n",
      "Epoch: 1, Batch: 350, Cost: 0.8799, Acc: 0.7344\n",
      "Test: 1, Cost: 0.7545, Acc: 0.7541\n",
      "*************************************\n",
      "Epoch: 2, Batch: 0, Cost: 0.5530, Acc: 0.8125\n",
      "Epoch: 2, Batch: 50, Cost: 0.6356, Acc: 0.8125\n",
      "Epoch: 2, Batch: 100, Cost: 0.7786, Acc: 0.8047\n",
      "Epoch: 2, Batch: 150, Cost: 0.7801, Acc: 0.7422\n",
      "Epoch: 2, Batch: 200, Cost: 0.6604, Acc: 0.7656\n",
      "Epoch: 2, Batch: 250, Cost: 0.7310, Acc: 0.7969\n",
      "Epoch: 2, Batch: 300, Cost: 0.6979, Acc: 0.7812\n",
      "Epoch: 2, Batch: 350, Cost: 0.6300, Acc: 0.8281\n",
      "Test: 2, Cost: 0.7311, Acc: 0.7620\n",
      "*************************************\n",
      "Epoch: 3, Batch: 0, Cost: 0.6425, Acc: 0.7891\n",
      "Epoch: 3, Batch: 50, Cost: 0.7419, Acc: 0.7500\n",
      "Epoch: 3, Batch: 100, Cost: 0.7207, Acc: 0.7500\n",
      "Epoch: 3, Batch: 150, Cost: 0.6049, Acc: 0.7969\n",
      "Epoch: 3, Batch: 200, Cost: 0.6652, Acc: 0.7734\n",
      "Epoch: 3, Batch: 250, Cost: 0.6209, Acc: 0.8047\n",
      "Epoch: 3, Batch: 300, Cost: 0.6314, Acc: 0.8047\n",
      "Epoch: 3, Batch: 350, Cost: 0.6304, Acc: 0.7812\n",
      "Test: 3, Cost: 0.7206, Acc: 0.7631\n",
      "*************************************\n",
      "Epoch: 4, Batch: 0, Cost: 0.5332, Acc: 0.8281\n",
      "Epoch: 4, Batch: 50, Cost: 0.4137, Acc: 0.8906\n",
      "Epoch: 4, Batch: 100, Cost: 0.5245, Acc: 0.8516\n",
      "Epoch: 4, Batch: 150, Cost: 0.5254, Acc: 0.8281\n",
      "Epoch: 4, Batch: 200, Cost: 0.4309, Acc: 0.8359\n",
      "Epoch: 4, Batch: 250, Cost: 0.6430, Acc: 0.8203\n",
      "Epoch: 4, Batch: 300, Cost: 0.5774, Acc: 0.8203\n",
      "Epoch: 4, Batch: 350, Cost: 0.5443, Acc: 0.8438\n",
      "Test: 4, Cost: 0.7149, Acc: 0.7651\n",
      "*************************************\n",
      "Epoch: 5, Batch: 0, Cost: 0.7134, Acc: 0.7969\n",
      "Epoch: 5, Batch: 50, Cost: 0.4530, Acc: 0.8359\n",
      "Epoch: 5, Batch: 100, Cost: 0.5251, Acc: 0.8359\n",
      "Epoch: 5, Batch: 150, Cost: 0.6142, Acc: 0.8125\n",
      "Epoch: 5, Batch: 200, Cost: 0.6225, Acc: 0.8281\n",
      "Epoch: 5, Batch: 250, Cost: 0.6024, Acc: 0.8047\n",
      "Epoch: 5, Batch: 300, Cost: 0.5615, Acc: 0.8203\n",
      "Epoch: 5, Batch: 350, Cost: 0.7083, Acc: 0.7656\n",
      "Test: 5, Cost: 0.7113, Acc: 0.7665\n",
      "*************************************\n",
      "Epoch: 6, Batch: 0, Cost: 0.6203, Acc: 0.8438\n",
      "Epoch: 6, Batch: 50, Cost: 0.6017, Acc: 0.7734\n",
      "Epoch: 6, Batch: 100, Cost: 0.6893, Acc: 0.7891\n",
      "Epoch: 6, Batch: 150, Cost: 0.5377, Acc: 0.8047\n",
      "Epoch: 6, Batch: 200, Cost: 0.7380, Acc: 0.7656\n",
      "Epoch: 6, Batch: 250, Cost: 0.5439, Acc: 0.8281\n",
      "Epoch: 6, Batch: 300, Cost: 0.7114, Acc: 0.7422\n",
      "Epoch: 6, Batch: 350, Cost: 0.5928, Acc: 0.7812\n",
      "Test: 6, Cost: 0.7094, Acc: 0.7663\n",
      "*************************************\n",
      "Epoch: 7, Batch: 0, Cost: 0.5168, Acc: 0.8203\n",
      "Epoch: 7, Batch: 50, Cost: 0.5883, Acc: 0.7969\n",
      "Epoch: 7, Batch: 100, Cost: 0.3755, Acc: 0.8906\n",
      "Epoch: 7, Batch: 150, Cost: 0.7051, Acc: 0.7812\n",
      "Epoch: 7, Batch: 200, Cost: 0.5616, Acc: 0.8125\n",
      "Epoch: 7, Batch: 250, Cost: 0.6111, Acc: 0.7812\n",
      "Epoch: 7, Batch: 300, Cost: 0.5068, Acc: 0.8594\n",
      "Epoch: 7, Batch: 350, Cost: 0.6845, Acc: 0.8125\n",
      "Test: 7, Cost: 0.7084, Acc: 0.7661\n",
      "*************************************\n",
      "Epoch: 8, Batch: 0, Cost: 0.6100, Acc: 0.8047\n",
      "Epoch: 8, Batch: 50, Cost: 0.6755, Acc: 0.7891\n",
      "Epoch: 8, Batch: 100, Cost: 0.5388, Acc: 0.8125\n",
      "Epoch: 8, Batch: 150, Cost: 0.7023, Acc: 0.8047\n",
      "Epoch: 8, Batch: 200, Cost: 0.3152, Acc: 0.9141\n",
      "Epoch: 8, Batch: 250, Cost: 0.6495, Acc: 0.8125\n",
      "Epoch: 8, Batch: 300, Cost: 0.5342, Acc: 0.8281\n",
      "Epoch: 8, Batch: 350, Cost: 0.6850, Acc: 0.7734\n",
      "Test: 8, Cost: 0.7093, Acc: 0.7660\n",
      "*************************************\n",
      "Epoch: 9, Batch: 0, Cost: 0.5218, Acc: 0.8281\n",
      "Epoch: 9, Batch: 50, Cost: 0.5734, Acc: 0.8125\n",
      "Epoch: 9, Batch: 100, Cost: 0.5406, Acc: 0.7969\n",
      "Epoch: 9, Batch: 150, Cost: 0.6603, Acc: 0.8281\n",
      "Epoch: 9, Batch: 200, Cost: 0.5794, Acc: 0.7891\n",
      "Epoch: 9, Batch: 250, Cost: 0.5699, Acc: 0.8125\n",
      "Epoch: 9, Batch: 300, Cost: 0.5086, Acc: 0.8203\n",
      "Epoch: 9, Batch: 350, Cost: 0.4394, Acc: 0.8438\n",
      "Test: 9, Cost: 0.7099, Acc: 0.7667\n",
      "*************************************\n"
     ]
    }
   ],
   "source": [
    "for epoch_id in range(EPOCH_NUM):\n",
    "    \n",
    "    # 进行训练\n",
    "    for batch_id, data in enumerate(train_reader()):\n",
    "        # 对于 train_reader 中的每次batch，执行exe.run()运行执行器训练\n",
    "        # 喂入每个batch的训练数据，fetch损失值、准确率\n",
    "        train_cost, train_acc = exe.run(\n",
    "                                        program = fluid.default_main_program(),\n",
    "                                        feed = feeder.feed(data),\n",
    "                                        fetch_list = [avg_cost, acc])\n",
    "        \n",
    "        # 每100个batch打印一次训练结果，一个batch包含128条数据\n",
    "        if batch_id % 50 == 0:\n",
    "            print('Epoch: {}, Batch: {}, Cost: {:.4f}, Acc: {:.4f}'.format(epoch_id, batch_id, float(train_cost), float(train_acc)))\n",
    "    \n",
    "    # 进行测试\n",
    "    test_costs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for batch_id, data in enumerate(test_reader()):\n",
    "        # 对于 test_reader 中的每次batch，执行exe.run()运行执行器训练\n",
    "        # 喂入每个batch的训练数据，fetch损失值、准确率\n",
    "        test_cost, test_acc = exe.run(\n",
    "                                        program = test_program,\n",
    "                                        feed = feeder.feed(data),\n",
    "                                        fetch_list = [avg_cost, acc])\n",
    "        \n",
    "        test_costs.append(test_cost[0])\n",
    "        test_accs.append(test_acc[0])\n",
    "        \n",
    "    # 计算每轮所有batch的误差平均值、误差准确率，然后输出   \n",
    "    test_cost = (sum(test_costs) / len(test_costs))\n",
    "    test_acc = (sum(test_accs) / len(test_accs))\n",
    "    print('Test: {}, Cost: {:.4f}, Acc: {:.4f}'.format(epoch_id, float(test_cost), float(test_acc)))\n",
    "    print(\"*************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明：\n",
    "- dirname(str): 保存预测模型的路径\n",
    "- feeded_var_names(list[str])：预测需要feed的数据\n",
    "- target_vars(list[variable])：保存预测结果的变量\n",
    "- executor(Executor)：保存预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"fc_0.tmp_4\"\n",
       "type {\n",
       "  type: LOD_TENSOR\n",
       "  lod_tensor {\n",
       "    tensor {\n",
       "      data_type: FP32\n",
       "      dims: -1\n",
       "      dims: 10\n",
       "    }\n",
       "    lod_level: 0\n",
       "  }\n",
       "}\n",
       "persistable: false"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练模型保存完成！\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "fluid.io.save_inference_model(dirname = model_save_dir,\n",
    "                             feeded_var_names = [words.name],\n",
    "                             target_vars = [model],\n",
    "                             executor = exe)\n",
    "\n",
    "print(\"训练模型保存完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 创建预测执行器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_place = fluid.CPUPlace()\n",
    "predict_exe = fluid.Executor(predict_place)\n",
    "predict_exe.run(fluid.default_startup_program())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 读取预测模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_inference_model() 函数的返回一个包含三个元素的元组\n",
    "- program 预测用的程序\n",
    "- feeded_var_names 一个str列表，包含需要在预测程序中提供数据的变量的名称\n",
    "- target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从模型中获取预测程序、输入数据名称列表、分类器\n",
    "[predict_program, predict_feeded_var_names, predict_target_var] = fluid.io.load_inference_model(\n",
    "                                                                                dirname = model_save_dir,\n",
    "                                                                                executor = predict_exe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"save_infer_model/scale_0\"\n",
       " type {\n",
       "   type: LOD_TENSOR\n",
       "   lod_tensor {\n",
       "     tensor {\n",
       "       data_type: FP32\n",
       "       dims: -1\n",
       "       dims: 10\n",
       "     }\n",
       "     lod_level: 0\n",
       "   }\n",
       " }\n",
       " persistable: false]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_target_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sentence):\n",
    "    # 读取数据字典文件\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f_data:\n",
    "        dict_txt = eval(f_data.readlines()[0])\n",
    "    \n",
    "    dict_txt = dict(dict_txt)\n",
    "    \n",
    "    # 把字符串数据转换成列表数据\n",
    "    keys = dict_txt.keys()\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for s in sentence:\n",
    "        if not s in keys:\n",
    "            s = '<unk>'\n",
    "        \n",
    "        data.append(int(dict_txt[s]))\n",
    "    \n",
    "    return np.array(data, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = [\"敦煌与人民大学共建敦煌文化学院两件流失海外的千年国宝将“回家”\",\n",
    "        \"徐峥发文悼念高以翔，并严厉斥责《追我吧》节目组\",\n",
    "        \"丁俊晖点出比赛转折点，期待英竞标赛赢下更多比赛\",\n",
    "        \"易会满署名文章信息量大，点名监管干部\",\n",
    "        \"11月26日甚至一手住宅成交129套涨幅约50%\",\n",
    "        \"原北汽新能源总经理郑刚正式加盟华为\",\n",
    "        \"学校作业多和课外班多，8成家长担忧孩子睡眠不足\",\n",
    "        \"科学家宣称发现宇宙中第五种基本了的证据\",\n",
    "        \"青瓦台前绝食8天后，韩总统热门人选突然不省人事\",\n",
    "        \"沪指低开0.03%，科技股活跃权重低迷\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news)):\n",
    "    data.append(get_data(news[i]))\n",
    "    \n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31, 23, 23, 18, 24, 17, 23, 19, 23, 19]]\n"
     ]
    }
   ],
   "source": [
    "# 获取每句话的单词数量\n",
    "base_shape = [[len(w) for w in data]]\n",
    "\n",
    "print(base_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成预测数据\n",
    "tensor_words = fluid.create_lod_tensor(data, base_shape, predict_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_exe.run(program = predict_program,\n",
    "                feed = {predict_feeded_var_names[0]: tensor_words},\n",
    "                fetch_list = predict_target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类名称\n",
    "names = [ '文化', '娱乐', '体育', '财经', '房产', '汽车', '教育', '科技', '国际', '证券']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "敦煌与人民大学共建敦煌文化学院两件流失海外的千年国宝将“回家”\n",
      "预测结果标签为： 0， 名称为：文化， 概率为：0.940224\n",
      "*******************\n",
      "徐峥发文悼念高以翔，并严厉斥责《追我吧》节目组\n",
      "预测结果标签为： 1， 名称为：娱乐， 概率为：0.932801\n",
      "*******************\n",
      "丁俊晖点出比赛转折点，期待英竞标赛赢下更多比赛\n",
      "预测结果标签为： 2， 名称为：体育， 概率为：0.999573\n",
      "*******************\n",
      "易会满署名文章信息量大，点名监管干部\n",
      "预测结果标签为： 3， 名称为：财经， 概率为：0.507758\n",
      "*******************\n",
      "11月26日甚至一手住宅成交129套涨幅约50%\n",
      "预测结果标签为： 4， 名称为：房产， 概率为：0.561365\n",
      "*******************\n",
      "原北汽新能源总经理郑刚正式加盟华为\n",
      "预测结果标签为： 7， 名称为：科技， 概率为：0.553036\n",
      "*******************\n",
      "学校作业多和课外班多，8成家长担忧孩子睡眠不足\n",
      "预测结果标签为： 6， 名称为：教育， 概率为：0.998530\n",
      "*******************\n",
      "科学家宣称发现宇宙中第五种基本了的证据\n",
      "预测结果标签为： 7， 名称为：科技， 概率为：0.879325\n",
      "*******************\n",
      "青瓦台前绝食8天后，韩总统热门人选突然不省人事\n",
      "预测结果标签为： 8， 名称为：国际， 概率为：0.581737\n",
      "*******************\n",
      "沪指低开0.03%，科技股活跃权重低迷\n",
      "预测结果标签为： 9， 名称为：证券， 概率为：0.683330\n",
      "*******************\n"
     ]
    }
   ],
   "source": [
    "# 获取概率最大的label\n",
    "for i in range(len(data)):\n",
    "    lab = np.argsort(result)[0][i][-1]\n",
    "    print(news[i])\n",
    "    print('预测结果标签为： {}， 名称为：{}， 概率为：{:.6f}'.format(lab, names[lab], result[0][i][lab]))\n",
    "    print(\"*******************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
